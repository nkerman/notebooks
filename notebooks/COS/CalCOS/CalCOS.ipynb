{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topC\"></a>\n",
    "\n",
    "# Running the COS Pipeline (`CalCOS`)\n",
    "\n",
    "# Learning Goals\n",
    "### This Notebook is designed to walk the user (*you*) through:\n",
    "#### 1. **[Setting up the environment to run `CalCOS`](#setupC)**\n",
    "##### - 1.1. [Prerequisites](#prereqC)\n",
    "##### - 1.2. [Create your conda environment](#condaenvC)\n",
    "##### - 1.3. [Set up a reference file directory](#lrefC)\n",
    "\n",
    "\n",
    "#### 2. **[Gathering the data to run `CalCOS`](#gatherC)**\n",
    "##### - 2.1. [Downloading the raw data](#datadlC)\n",
    "##### - 2.2. [Gathering reference files](#reffileC)\n",
    "\n",
    "#### 3. **[Processing raw COS data using `CalCOS`](#runC)**\n",
    "##### - 3.1. [Running `CalCOS`: *From a python environment*](#runpyC)\n",
    "##### - 3.2. [Running `CalCOS`: *From the command line*](#runcliC)\n",
    "\n",
    "#### 4. **[Re-processing COS data with altered parameters](#rerunC)**\n",
    "##### - 4.1. [Altering the calibration switches](#alterswitchC)\n",
    "##### - 4.2. [Running `CalCOS` with a specific set of switches](#switchrunC)\n",
    "##### - 4.3. [Running `CalCOS` with a different reference file](#refrunC)\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "#### The Cosmic Origins Spectrograph ([*COS*](https://www.nasa.gov/content/hubble-space-telescope-cosmic-origins-spectrograph)) is an ultraviolet spectrograph on-board the Hubble Space Telescope ([*HST*](https://www.stsci.edu/hst/about)) with capabilities in the near ultraviolet (*NUV*) and far ultraviolet (*FUV*).\n",
    "\n",
    "#### **`CalCOS`** is the data processing pipeline which converts the raw data produced by COS's detectors onboard Hubble into usable spectral data. It transitions the data from a list of many individual recorded photon interactions into tables of wavelength and flux.\n",
    "#### This tutorial aims to prepare you run the `CalCOS` pipeline to reduce spectral data taken with the COS instrument. It focuses on COS data taken in `TIME-TAG` mode. \n",
    "*Note* that there is another, less commonly-used mode: `ACCUM`, which should only be used for UV bright targets\n",
    "\n",
    "- For an in-depth manual to working with COS data and a discussion of caveats and user tips, see the [COS Data Handbook](https://hst-docs.stsci.edu/display/COSDHB/).\n",
    "- For a detailed overview of the COS instrument, see the [COS Instrument Handbook](https://hst-docs.stsci.edu/display/COSIHB/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes for those new to Python/Jupyter/Coding:\n",
    "- You will frequently see exclamation points (**!**) or dollar signs (**\\$**) at the beginning of a line of code. These are not part of the actual commands. The exclamation points tell a jupyter notebook to pass the following line to the command line, and the dollar sign merely indicates the start of a terminal prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will import the following packages:\n",
    "\n",
    "- `astroquery.mast Mast and Observations` for finding and downloading data from the [MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html) archive\n",
    "- `numpy` to handle array functions (version $\\ge$ 1.17)\n",
    "- `astropy.io` fits for accessing FITS files\n",
    "- `astropy.table Table` for creating tidy tables of the data\n",
    "<!-- - `astropy.units` and `astropy.visualization.quantity_support` for dealing with units -->\n",
    "- `matplotlib.pyplot` for plotting data\n",
    "- `glob` and `os` for searching and working with system files\n",
    "\n",
    "- *Later on, we will import the `calcos` package to run the COS data pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is just to make any plots we might make look good in a notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Manipulating arrays\n",
    "import numpy as np\n",
    "\n",
    "# Reading in data\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Downloading data from archive\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# Searching for files on our system\n",
    "import glob\n",
    "# Making environment variables\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will also define a few directories in which to place our data and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be important directories for the notebook\n",
    "cwd = !pwd\n",
    "cwd = cwd[0]\n",
    "\n",
    "!mkdir ./data\n",
    "!mkdir ./output/\n",
    "\n",
    "datadir = cwd + '/data/'\n",
    "output_dir = cwd + '/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = setupC></a>\n",
    "# 1. Setting up the environment to run `CalCOS`\n",
    "\n",
    "The first step to processing your data is setting up an environment from which to run `CalCOS`.\n",
    "<a id = prereqC></a>\n",
    "## 1.1. Prerequisites\n",
    "This tutorial assumes some basic knowledge of the command line and was built using a unix bash-style shell. Those using a Windows computer will likely have the best results if working within the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10).\n",
    "\n",
    "\n",
    "If you do not already have any distribution of the `conda` tool, see [this page](https://astroconda.readthedocs.io/en/latest/getting_started.html#getting-started-jump) for instructions, and install either [`anaconda` - more beginner friendly, \\~ 3 GB, lots of extras you likely won't use](https://docs.anaconda.com/anaconda/install/) or [`miniconda` - \\~ 400 MB, only what you need](https://docs.conda.io/en/latest/miniconda.html).\n",
    "\n",
    "<a id = condaenvC></a>\n",
    "## 1.2. Create your conda environment\n",
    "Once you have `conda` installed, it's time to create an environment. If you already setup an `astroconda` environment in the notebook on \"Setting up your environment\", you may skip this and merely activate the environment you created then.\n",
    "\n",
    "Open up your terminal app, likely `Terminal` or `iTerm` on a Mac or `Windows Terminal` or `Powershell` on Windows.\n",
    "\n",
    "First, add the stsci channel to your computer's conda channel list. This enables conda to look in the right place to find all the packages we want to install.\n",
    "\n",
    "\n",
    "``` $ conda config --add channels http://ssb.stsci.edu/astroconda```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new environment for running `CalCOS`; let's call it `calcos_env`, and initialize it with the packages in the stsci channel's list we just added.\n",
    "\n",
    "\n",
    "``` $ conda create -n calcos_env stsci ```\n",
    "\n",
    "After allowing conda to proceed to installing the packages (type `y` then hit enter/return), you can see all of your environments with:\n",
    "\n",
    "``` $ conda env list```\n",
    "\n",
    "and then switch over to your new environment with \n",
    "\n",
    "``` $ conda activate calcos_env ```\n",
    "\n",
    "At this point, typing `calcos` into the command line and hitting enter should no longer yield the error \n",
    "\n",
    "> ```command not found: calcos``` \n",
    "\n",
    "but rather respond that:\n",
    "\n",
    "> ```The command-line options are:\n",
    "  --version (print the version number and exit)\n",
    "  -r (print the full version string and exit)\n",
    "  ...\n",
    "  ERROR:  An association file name or observation rootname must be specified.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = lrefC></a>\n",
    "## 1.3. Set up a reference file directory\n",
    "\n",
    "`CalCOS` needs to be able to find all your reference files, (flat field image, bad pixel table, etc.), and the best way to enable that is to create a central directory of all the calibration files you'll need. We refer to this directory  as \"lref\" by convention, and set a system variable `lref` to the location of the directory. In this section, we will create the `lref` environment variable; however, we need to populate the `lref` folder with the actual reference files. We do this in [Section 2.2](#reffileC). If you have already downloaded the set of COS reference files you need to use into an existing lref directory, you should instead set `lref` to the path to this directory. \n",
    "\n",
    "We can assign a system variable in three different ways, depending on whether we are working from:\n",
    "1. The command line\n",
    "2. A python environment\n",
    "3. A Jupyter Notebook\n",
    "\n",
    "|Unix-style Command Line| Python | Jupyter Notebook|\n",
    "|-|-|-|\n",
    "| export lref='./data/reference/...' | import os| %env lref ./data/reference/...|\n",
    "||os.environ[\"lref\"] = \"./data/reference/...\" ||\n",
    "\n",
    "\n",
    "Note that this system variable must be set again with every new instance of a terminal - if you frequently need to use the same `lref` directory, consider adding an export statement to your `.bash_profile` or equivalent file.\n",
    "\n",
    "Because this is a jupyter notebook, we set our reference directory with the [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) below:\n",
    "\n",
    "<!-- Looking in the headers of our data below, we see that the `$lref` argument appears at the beginning of all of the reference file locations: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env lref ./data/reference/references/hst/cos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note the value of the system variable using the `echo` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $lref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = gatherC></a>\n",
    "# 2. Gathering the data to run `CalCOS`\n",
    "\n",
    "The `CalCOS` pipeline can be run either from a python environment, or directly from a Unix-style command line. The two use the same underlying machinery but can differ in syntax. For specifics on the keywords to run `CalCOS` with specific behaviors and arguments, see [Table 3.2: Arguments for Running CalCOS in Python](https://hst-docs.stsci.edu/cosdhb/chapter-3-cos-calibration/3-6-customizing-cos-data-calibration) and [Table 3.3: Command-line Options for Running CalCOS in Unix/Linux/Mac](https://hst-docs.stsci.edu/cosdhb/chapter-3-cos-calibration/3-6-customizing-cos-data-calibration).\n",
    "\n",
    "<a id = datadlC></a>\n",
    "## 2.1. Downloading the raw data\n",
    "\n",
    "First, we need to make sure we have all of our data ready and in the right spot. If you are unfamiliar with searching the archive for data, we recommend that you view our [tutorial on downloading COS data](https://github.com/spacetelescope/COS-Notebooks). This notebook will largely gloss over downloading the data.\n",
    "\n",
    "To run `CalCOS`, we will need the following files:\n",
    "1. All the **raw data** from separate exposures we wish to combine as `_rawtag` fits files\n",
    "2. The **association** file telling `CalCOS` which files to combine as a `_asn` fits file.\n",
    "\n",
    "##### *Note* that we do not generally run the `CalCOS` pipeline directly on the data files, but instead on an association `_asn` file. This allows for the calibration of related exposures into combined `_x1dsum` files.\n",
    "If you instead use `_rawtag` or `_corrtag` exposure files files as your inputs, you will only receive the exposure-specific `_x1d` files as your outputs.\n",
    "\n",
    "##### For this example, we're choosing the dataset `LCXV13040` of COS/FUV observing the [quasar 3C48](https://en.wikipedia.org/wiki/3C_48). In the cell below we download the data from the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the MAST archive for data with observation id starting with lcxv1304\n",
    "q1 = Observations.query_criteria(obs_id = 'lcxv1304*')\n",
    "\n",
    "# Get a list of all products we could download associates with this file\n",
    "pl = Observations.get_product_list(q1)\n",
    "\n",
    "# Get a list of only the products which are association files\n",
    "asn_file_list = pl[pl[\"productSubGroupDescription\"] == 'ASN']\n",
    "\n",
    "# Get a list of only the products which are rawtag files\n",
    "rawtag_list = pl[(pl[\"productSubGroupDescription\"] == 'RAWTAG_A') | (pl[\"productSubGroupDescription\"] == 'RAWTAG_B')]\n",
    "\n",
    "#Download the two lists to the data directory\n",
    "Observations.download_products(rawtag_list, download_dir='./data')\n",
    "Observations.download_products(asn_file_list, download_dir='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, each exposure's files are downloaded to separate directories, as is the association file.\n",
    "We need to move around these files to all be in the same directory, which we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./data/mastDownload/HST/lcxv13*/*fits* ./data # move the files to the base data directory\n",
    "!rm -r ./data/mastDownload # Delete the now-empty nested subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = reffileC></a>\n",
    "## 2.2. Gathering reference files\n",
    "\n",
    "#### The following process is given a detailed explanation in [Section 3 of our notebook on \"Setting up your environment\"](https://spacetelescope.github.io/COS-Notebooks/Setup.html#crdsS). Your path will be somewhat simpler if you have already downloaded the reference files.\n",
    "\n",
    "Each data file has an associated set of calibration files which are needed to run the associated correction with (i.e. you need the `FLATFILE` to flat field correct the data.) These reference files must be located in the `$lref` directory to run the pipeline.\n",
    "\n",
    "The Space Telescope Science Institute (STScI) team is regularly producing new calibration files, in an effort to keep improving data reduction. Periodically the pipeline is re-run on all COS data.\n",
    "To determine which reference files were used most recently by STScI to calibrate your data (often, but not always the newest and best,) you can refer to your data file's \"CRDS_CTX\" keyword in its fits header (see next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawfiles = glob.glob('data/*raw*.fits') # find all of the raw files\n",
    "crds_ctx = fits.getheader(rawfiles[0])['CRDS_CTX'] # Get the header of the 0th raw file, look for its CRDS context keyword\n",
    "print(f\"The CRDS Context last run with {rawfiles[0]} was:\\t{crds_ctx}\") # print it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of this keyword in the header is a `.pmap` file which tells the CRDS calibration data distribution program which files to distribute. To download the reference files specified by the context, we use the tool `crds`, installed with the stsci conda channel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### If you ran Section 3 of the \"Setup\" notebook, you likely do not have to download more reference files. \n",
    "### [If you haven't downloaded the files as in Section 3 of \"Setup.ipynb\", click to skip this cell!](#skipcellC)\n",
    "#### You can instead simply point to the ones you downloaded, using the `crds bestrefs` command, as shown in the following three steps. Run these steps from your command line if and only if you already have the reference files in a local cache.\n",
    "\n",
    "1. The following sets environment variable for crds to look for the reference data online:\n",
    "\n",
    "```$ export CRDS_SERVER_URL=https://hst-crds.stsci.edu``` \n",
    "\n",
    "2. The following tells crds where to save the files it downloads - set this to the directory where you saved the crds_cache as in [Section 3 of our notebook on \"Setup\"](https://spacetelescope.github.io/COS-Notebooks/Setup.html#crdsS):\n",
    "\n",
    "```$ export CRDS_PATH=${HOME}/crds_cache```\n",
    "\n",
    "3. The following will update the data files you downloaded so that they will be processed with the reference files you previously downloaded:\n",
    "\n",
    "```$ crds bestrefs --files data/*raw*.fits --update-bestrefs --new-context '<the imap or pmap file you used to download the reference files>'```\n",
    "\n",
    "#### Assuming everything ran successfully, you can now [skip to Section 3](#runC).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=skipcellC></a>\n",
    "### If you have not yet downloaded the reference files, you will need to do so, as shown below:\n",
    "\n",
    "#### Caution!\n",
    "<img src= ./figures/warning.png width =\"60\" title=\"CAUTION!\"> \n",
    "\n",
    "*Note* that as of the time of this notebook's initial creation, the pipeline context used below was **`hst_0836.pmap`**, but this changes over time. You are running this in the future, and there is very possibly a newer context you would be better off working with. Take a minute to consider this, and check the [HST Calibration Reference Data System webpage](http://hst-crds.stsci.edu/) to determine what the currently operational pmap file is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unless we are connected to the STScI network, or already have the reference files on our machine, we will need to download the reference files and tell the pipeline where to look for the flat files, bad-pixel files, etc.\n",
    "\n",
    "#### Caution again!\n",
    "<img src=figures/warning.png width =\"60\" title=\"CAUTION!\"> <img src=figures/warning.png width =\"60\" title=\"CAUTION!\"> \n",
    "\n",
    "#### The process in the following two cells can take a long time and strain network resources! If you have already downloaded *up-to-date* COS reference files, avoid doing so again.\n",
    "Instead, keep these crds files in an accessible location, and point an environment variable `lref` to this directory. For instance, if your `lref` files are on your username's home directory in a subdirectory called `crds_cache`, give Jupyter the following command then skip to [Section 2.3](#runpyC):\n",
    "\n",
    "```%env lref /Users/<your username>/crds_cache/references/hst/cos/```\n",
    "\n",
    "#### Only run the following three code cells if you have not downloaded these files before:\n",
    "In the next two cells, we will setup an environment of reference files, download the files, and save the output of the crds download process in a log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr \n",
    "# The above ^ allows us to redirect LOTS of output into a txt file in the next cell, rather than have a huge printed output\n",
    "%env CRDS_SERVER_URL https://hst-crds.stsci.edu\n",
    "# The above ^ sets environment variable for crds to look for the reference data online\n",
    "%env CRDS_PATH ./data/reference/ \n",
    "#The above ^ tells crds where to save the files it downloads\n",
    "\n",
    "# The next command depends on your context and pmap file  - it looks up the specified pmap \"context\" file,\n",
    "    # which tells it what reference files to download. It then downloads these to the CRDS_PATH directory\n",
    "# You may wish to update this if there is a newer pmap file - check https://hst-crds.stsci.edu\n",
    "!crds bestrefs --files data/*raw*.fits  --sync-references=2 --update-bestrefs --new-context 'hst_0867.pmap' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+'crds_output_1.txt', 'w') as f: # This file now contains the output of the last cell\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll print the beginning and end of that file just to take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crds_output_dict = {} # pair each line with its line number, start at 0\n",
    "with open('./output/crds_output_1.txt', 'r') as cell_outputs: # open the file\n",
    "    for linenum, line in enumerate(cell_outputs): # loop through lines\n",
    "        crds_output_dict[linenum] = line[:-1] # save each line to dict\n",
    "total_lines = len(crds_output_dict) # Get the length of the dictionary - how many lines of output\n",
    "\n",
    "print(f\"Printing the first and last 5 lines of {total_lines} lines output by the previous cell:\\n\")    \n",
    "for i in np.append(range(5), np.subtract(total_lines - 1, range(5)[::-1] )):\n",
    "    print(f\"Line {i}:   \\t\", crds_output_dict[i])\n",
    "\n",
    "crds_output_dict.clear() # Delete the contents of the dict to avoid 'garbage' piling up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Line 158 of the output should show 0 errors. \n",
    "If you receive errors, you may need to attempt to run the `crds bestrefs` line again. These errors can arise from imperfect network connections. \n",
    "##### It is recommended that you use this new `$lref` folder of reference files for subsequent `CalCOS` use, rather than re-downloading the reference files each time. To do this (*after completing this notebook*): \n",
    "- Save this folder somewhere accessibile, i.e. `~/crds_cache`\n",
    "- Add a line to your .bashrc or similar: `export lref=$HOME'/crds_cache'`\n",
    "  - If you wish to avoid adding this to your .bashrc, simply type the line above into any terminal you wish to run `CalCOS` from\n",
    "  - If running `CalCOS` from a jupyter notebook, add a cell with: `%env lref /Users/<Your Username>/crds_cache/references/hst/cos`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = runC></a>\n",
    "# 3. Processing raw COS data using `CalCOS`\n",
    "\n",
    "### Now we can run the pipeline on our data using our reference files\n",
    "\n",
    "This following cells running the pipeline can take several minutes, sometimes more than **10 minutes**, so you may choose to not run the remaining cells of this notebook on the example data. You may, instead, wish to simply look at the rendered output in the [html version of this notebook](https://spacetelescope.github.io/COS-Notebooks/Calcos.html).\n",
    "\n",
    "By default, the pipeline also outputs hundreds of lines of text - we will suppress the printing of this text and instead save it to a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = runpyC></a>\n",
    "## 3.1. Running `CalCOS`: *From a python environment*\n",
    "\n",
    "First, we import the pipeline package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calcos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can run the pipeline program!\n",
    "\n",
    "Note that generally, `CalCOS` should be run on an association (`_asn`) file (in this case: `./data/lcxv13040_asn.fits`). You *may* run CalCOS directly on `_rawtag` or `_corrtag` exposure files, but this will not produce an `_x1dsum` file. No matter what type of files you run CalCOS on, you should only specify the FUVA segment's file, i.e. the `_rawtag_a` file.\n",
    "\n",
    "In this example, we also specify that `verbosity` = 2, resulting in a **very** verbose output, and we specify a directory to put all the output files in: `output/calcos_processed_1`. To avoid polluting this notebook with more than a thousand lines of the output, we capture the output of the next cell and save it to `output/output_calcos_1.txt` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr \n",
    "# Above ^ again, capture the output and save it in the next cell\n",
    "\n",
    "calcos.calcos('./data/lcxv13040_asn.fits', # 1st param specifies which asn file to run the pipeline on\n",
    "              verbosity=2, # verbosity param: [0 = don't print much at all to the console or text file, 1 = print some, 2 = print everything]\n",
    "              outdir=output_dir+\"/calcos_processed_1\") # save all resulting files in this subdirectory in our output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+'output_calcos_1.txt', 'w') as f: # This file now contains the output of the last cell\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Again, we'll print the beginning and end of that file just to take a look and make sure `CalCOS` ran successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcos_output_dict = {} # pair each line with its line number, start at 0\n",
    "with open('./output/output_calcos_1.txt', 'r') as cell_outputs: # open the file\n",
    "    for linenum, line in enumerate(cell_outputs): # loop through lines\n",
    "        calcos_output_dict[linenum] = line[:-1] # save each line to dict\n",
    "total_lines = len(calcos_output_dict) # Get the length of the dictionary - how many lines of output\n",
    "\n",
    "print(f\"Printing the first and last 5 lines of {total_lines} lines output by the previous cell:\\n\")    \n",
    "for i in np.append(range(5), np.subtract(total_lines - 1, range(5)[::-1] )):\n",
    "    print(f\"Line {i}:   \\t\", calcos_output_dict[i])\n",
    "\n",
    "calcos_output_dict.clear() # Delete the contents of the dict to avoid 'garbage' piling up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = runcliC></a>\n",
    "\n",
    "## 3.2. Running `CalCOS`: *From the command line*\n",
    "\n",
    "The syntax for running `CalCOS` from the command line is very similar. Assuming your data files, `lref` directory, and reference files are all in order, you can simply run:\n",
    "\n",
    "```calcos --outdir directory_to_save_outputs_in filename_asn.fits```\n",
    "\n",
    "*or, if you want to save a very verbose output to a log file `log.txt`*:\n",
    "\n",
    "```calcos -v --outdir directory_to_save_outputs_in filename_asn.fits > log.txt```\n",
    "\n",
    "To see the full list of commands, [Table 3.2:Command-line Options for Running CalCOS in Unix/Linux/Mac](https://hst-docs.stsci.edu/cosdhb/chapter-3-cos-calibration/3-6-customizing-cos-data-calibration), or run the following cell with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!calcos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = rerunC></a>\n",
    "# 4. Re-processing COS data with altered parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = alterswitchC></a>\n",
    "## 4.1. Altering the calibration switches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The way to tweak how `CalCOS` runs - i.e. which calibrations it performs - is with the calibration switches contained in the fits headers. \n",
    "##### The switches (with the exception of \"XTRACTALG\"), can be set to:\n",
    "\n",
    "|***Value:***|\"PERFORM\"|\"OMIT\"|\"N/A\"|\n",
    "|-|-|-|-|\n",
    "|***Meaning:***|Performs the calibration step|Does not perform the calibration step|This step would not make sense for this file|\n",
    "\n",
    "`XTRACTALG` instead can be set to either \"BOXCAR\" or \"TWOZONE\", to specify the spectral extraction algorithm to be used. For more information, see [Section 3.2.1: \"Overview of TWOZONE extraction\" of the Data Handbook](https://hst-docs.stsci.edu/cosdhb/chapter-3-cos-calibration/3-2-pipeline-processing-overview#id-3.2PipelineProcessingOverview-3.2.1OverviewofTWOZONEextraction).\n",
    "\n",
    "In the cell below, we get a full list of the switches by name. If you to learn more about the calibration steps and switches, see [Chapters 2 and 3 of the COS Data Handbook](https://hst-docs.stsci.edu/cosdhb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = fits.getheader(rawfiles[0]) # gets header of one of the 0th rawfile\n",
    "calibswitches = header[82:109] # The calib switches are found in lines 82 - 109 of the header\n",
    "calibswitches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's begin by switching off all the switches currently set to \"PERFORM\" to a new value of \"OMIT\", in every rawfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False # Set to True to see a bit more about what is going on here\n",
    "\n",
    "for i, rawfile in enumerate(rawfiles): # Find each rawfile, i is just a counter variable for the files you loop through\n",
    "    if verbose:\n",
    "        print(rawfile)\n",
    "    header = fits.getheader(rawfiles[i]) # Get that rawfiles header\n",
    "    corrections = [key for key in list(header.keys()) if \"CORR\" in key] # Find all calib switches\n",
    "\n",
    "    for correction in corrections:\n",
    "        if header[correction] == 'PERFORM':\n",
    "            if verbose:\n",
    "                print(\"switching\\t\", header[correction], \"\\t\",correction, \"\\tto OMIT\")\n",
    "            fits.setval(rawfile, correction, value='OMIT', ext = 0) # Turn off all the calib switches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CalCOS` realizes that all the switches are set to \"OMIT\", and exits without doing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcos.calcos('./data/lcxv13040_asn.fits', verbosity=0, outdir=output_dir+\"calcos_processed_2\") # Run CalCOS with all calib switches OFF; allow text output this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = switchrunC></a>\n",
    "## 4.2. Running `CalCOS` with a specific set of switches\n",
    "Now, let's set a single switch to \"PERFORM\", and just run a flat-field correction (\"FLATCORR\") and a pulse-height filter correction (\"PHACORR\"). Set verbosity = 1 or 2 to learn more about how `CalCOS` is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "for i, rawfile in enumerate(rawfiles): # Find each rawfile, i is just a counter variable for the files you loop through\n",
    "    if verbose:\n",
    "        print(rawfile)\n",
    "    fits.setval(rawfile, \"FLATCORR\", value='PERFORM', ext = 0) # Change the header's keyword FLATCORR to the value PERFORM\n",
    "    fits.setval(rawfile, \"PHACORR\", value='PERFORM', ext = 0) # Change the header's keyword PHACORR to the value PERFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "calcos.calcos('./data/lcxv13040_asn.fits', verbosity=2, outdir=output_dir+\"calcos_processed_3\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+'output_calcos_3.txt', 'w') as f: # This file now contains the output of the last cell\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = refrunC></a>\n",
    "## 4.3. Running `CalCOS` with a different reference file\n",
    "\n",
    "You may wish to run `CalCOS` with a specific flat file, bad pixel table, or any other reference file. `CalCOS` offers the ability to do just this on a file-by-file basis, by changing the CALIBRATION REFERENCE FILES values in the header of your data.\n",
    "\n",
    "As an example, we check which calibration files are selected for one of our rawtag files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = fits.getheader(rawfiles[0]) # Get 0th rawfile's header\n",
    "refFiles = header[110:138] # The 110th to 138th lines of the header are filled with these reference files\n",
    "refFile_keys = list(refFiles[2:].keys()) # Get just the keywords i.e. \"FLATFILE\" and \"DEADTAB\"\n",
    "refFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's download another Pulse Height Amplitude (\\_pha) table file using the `crds` tool (*I arbitrarily picked this one*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!crds sync --files u1t1616ll_pha.fits --output-dir $lref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we can use the fits headers to set this new file as the `_pha` file. As a demonstration, let's do this for **only the raw data from segment FUVA** of the FUV detector:\n",
    "*Note* that we are still only performing two corrections, as all calibration switches aside from `FLATCORR` and `PHACORR` are set to `OMIT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawfiles_segA = glob.glob('data/*rawtag_a*.fits') # Find just the FUVA raw files\n",
    "for rawfileA in rawfiles_segA:\n",
    "    print(rawfileA)\n",
    "    with fits.open(rawfileA, mode = 'update') as hdulist:\n",
    "        hdr0 = hdulist[0].header # Update the 0th header of that FUVA file\n",
    "        hdr0[\"PHATAB\"] = 'lref$u1t1616ll_pha.fits' #NOTE that you need the $lref in there if you put it with your other ref files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, let's run `CalCOS` with the new `_pha` file for only the FUVA data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "calcos.calcos('./data/lcxv13040_asn.fits', verbosity=2, outdir=output_dir+\"/calcos_processed_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir+'output_calcos_4.txt', 'w') as f: # This file now contains the output of the last cell\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we go, let's have a look at the spectra we calibrated and extracted in part [2.3](#runpyC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll make a very quick plot to show the spectra calibrated by STScI's pipeline and by us right now.\n",
    "Much more information on reading in and plotting COS spectra can be found in our other tutorial: [Viewing COS Data](https://spacetelescope.github.io/COS-Notebooks/ViewData.html).\n",
    "\n",
    "*(You can ignore the UnitsWarning below)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the STScI calibrated x1dsum spectrum from the archive\n",
    "Observations.download_products(Observations.get_product_list(Observations.query_criteria(obs_id = 'lcxv13040')), mrp_only=True,  download_dir = 'data/compare/')\n",
    "# Read in this spectrum\n",
    "output_spectrum = Table.read('./data/compare/mastDownload/HST/lcxv13040/lcxv13040_x1dsum.fits')\n",
    "wvln_orig, flux_orig, fluxErr_orig, dqwgt_orig = output_spectrum[1][\"WAVELENGTH\", \"FLUX\", \"ERROR\" ,\"DQ_WGT\"]\n",
    "dqwgt_orig = np.asarray(dqwgt_orig, dtype=bool) # Convert the data quality (DQ) weight into a boolean we can use to mask the data\n",
    "# Also read in the spectrum we calibrated in Section 2.3\n",
    "output_spectrum = Table.read('output/calcos_processed_1/lcxv13040_x1dsum.fits')\n",
    "new_wvln, new_flux, new_fluxErr, new_dqwgt = output_spectrum[1][\"WAVELENGTH\", \"FLUX\", \"ERROR\" ,\"DQ_WGT\"]\n",
    "new_dqwgt = np.asarray(new_dqwgt, dtype=bool) # Convert the data quality (DQ) weight into a boolean we can use to mask the data\n",
    "\n",
    "fig, (ax0,ax1, ax2) = plt.subplots(3,1,figsize=(15,10)) # 3 row x 1 column figure\n",
    "ax0.plot(wvln_orig[dqwgt_orig], flux_orig[dqwgt_orig], linewidth = 0.5, c = 'C0', label = \"Processed by the archive\") # Plot the archive's spectrum in top section\n",
    "ax1.plot(new_wvln[new_dqwgt], new_flux[new_dqwgt], linewidth = 0.5, c = 'C1', label = \"Just now processed by you\") # Plot your calibrated spectrum in middle section\n",
    "\n",
    "ax2.plot(wvln_orig[dqwgt_orig], flux_orig[dqwgt_orig], linewidth = 0.5, c = 'C0', label = \"Processed by the archive\") # Plot both spectra in bottom section\n",
    "ax2.plot(new_wvln[new_dqwgt], new_flux[new_dqwgt], linewidth = 0.5, c = 'C1', label = \"Just now processed by you\")\n",
    "\n",
    "ax0.legend(loc = 'upper center',fontsize = 14);ax1.legend(loc = 'upper center',fontsize = 14); ax2.legend(loc = 'upper center',fontsize = 14)\n",
    "ax0.set_title(\"Fig 3.1\\nComparison of processed spectra\",size = 28)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + \"fig3.1_compare_plot.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You finished this notebook!\n",
    "### There are more COS data walkthrough notebooks on different topics. You can find them [here](https://spacetelescope.github.io/COS-Notebooks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## About this Notebook\n",
    "**Author:** Nat Kerman <nkerman@stsci.edu>\n",
    "\n",
    "**Updated On:** 2021-1-8\n",
    "\n",
    "\n",
    "> *This tutorial was generated to be in compliance with the [STScI style guides](https://github.com/spacetelescope/style-guides) and would like to cite the [Jupyter guide](https://github.com/spacetelescope/style-guides/blob/master/templates/example_notebook.ipynb) in particular.*\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `astropy`, `matplotlib`, `astroquery`, or `numpy` for published research, please cite the\n",
    "authors. Follow these links for more information about citations:\n",
    "\n",
    "* [Citing `astropy`/`numpy`/`matplotlib`](https://www.scipy.org/citing.html)\n",
    "* [Citing `astroquery`](https://astroquery.readthedocs.io/en/latest/)\n",
    "\n",
    "---\n",
    "\n",
    "[Top of Page](#topC)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> \n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
