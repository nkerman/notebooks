{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topAF\"></a>\n",
    "\n",
    "# Modifying or Creating an Association File\n",
    "\n",
    "# Learning Goals\n",
    "### This Notebook is designed to walk the user (*you*) through: **Creating or altering the association (`asn`) file used by the Cosmic Origins Spectrograph (*COS*) pipeline to determine which data to process**:\n",
    "   #### 1. [**Examining an association file**](#examAF)\n",
    "   #### 2. [**Editing an existing association file**](#editAF)\n",
    "   ##### - 2.1. [Removing an exposure](#subAF)\n",
    "   ##### - 2.2. [Adding an exposure](#addAF)\n",
    "   #### 3. [**Creating an entirely new association file**](#newAF)\n",
    "   ##### - 3.1. [Simplest method](#simpleAF)\n",
    "   ##### - 3.2. [With fits header metadata](#metaAF)\n",
    "<!--    #### 3. [**Reprocessing the data**](#calcosAF) **Optional** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "#### The Cosmic Origins Spectrograph ([*COS*](https://www.nasa.gov/content/hubble-space-telescope-cosmic-origins-spectrograph)) is an ultraviolet spectrograph on-board the Hubble Space Telescope ([*HST*](https://www.stsci.edu/hst/about)) with capabilities in the near ultraviolet (*NUV*) and far ultraviolet (*FUV*). \n",
    "\n",
    "#### This tutorial aims to prepare you to alter the association file used by the `calcos` pipeline. \n",
    "\n",
    "- For an in-depth manual to working with COS data and a discussion of caveats and user tips, see the [COS Data Handbook](https://hst-docs.stsci.edu/display/COSDHB/).\n",
    "- For a detailed overview of the COS instrument, see the [COS Instrument Handbook](https://hst-docs.stsci.edu/display/COSIHB/).\n",
    "\n",
    "#### We'll demonstrate creating an `asn` file in two ways:\n",
    "##### 1. Editing an existing `asn` file to add or remove an exposure\n",
    "##### 2. Creating an entirely new `asn` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will import the following packages:\n",
    "\n",
    "- numpy to handle array functions\n",
    "- astropy.io fits and astropy.table Table for accessing FITS files\n",
    "- glob, os, and shutil for working with system files\n",
    "- astroquery.mast Mast and Observations for finding and downloading data from the [MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html) archive\n",
    "- datetime for updating fits headers with today's date\n",
    "\n",
    "These python packages are installed standard with the the STScI conda distribution. For more information, see our notebook tutorial on [setting up an environment](https://github.com/spacetelescope/COS-Notebooks/blob/master/Setup/Setup.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for array manipulation\n",
    "import numpy as np\n",
    "# for reading fits files\n",
    "from astropy.io import fits                                            \n",
    "from astropy.table import Table\n",
    "# for system files\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "# for downloading the data\n",
    "from astroquery.mast import Observations\n",
    "# for changing today's date in a fits header\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will also define a few directories we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'\n",
    "output_dir = './output/'\n",
    "plots_dir = output_dir + 'plots/'\n",
    "# Make the directories in case they don't exist\n",
    "!mkdir ./data\n",
    "!mkdir ./output \n",
    "!mkdir ./output/plots/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we will need to download the data we wish to filter and analyze\n",
    "We choose the exposures with the association obs_ids: `ldif01010` and `ldif02010` because we know that some of the exposures in these groups failed. For more information on downloading COS data, see our [notebook tutorial on downloading COS data](https://github.com/spacetelescope/COS-Notebooks/blob/master/DataDL/DataDl.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Observations.get_product_list(Observations.query_criteria(obs_id = 'ldif0*10')) # search for the correct obs_ids and get the product list \n",
    "arr = []\n",
    "[arr.append(i) for i, psg in enumerate(pl['productSubGroupDescription']) if psg in ['RAWTAG_A', 'RAWTAG_B','ASN']] # get the indices of rawtag and asn files in the product list\n",
    "Observations.download_products(pl[arr], download_dir = './data/') # Download these chosen products\n",
    "for gfile in glob.glob(\"**/ldif*/*.fits\", recursive=True): # Move all fits files in this set to the base data directory\n",
    "    os.rename(gfile,data_dir + os.path.basename(gfile))\n",
    "shutil.rmtree(data_dir + 'mastDownload') # Delete the empty nested mastDownload directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = examAF></a>\n",
    "# 1. Examining an association file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we downloaded two association files and their rawtag data files. We will begin by searching for the association files and reading one of them (`LDIF01010`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asnfiles = glob.glob(\"**/*ldif*asn*\", recursive=True) # There will be two (ldif01010_asn.fits and ldif02010_asn.fits)\n",
    "asnfile = asnfiles[0] # We want to work primarily with ldif01010_asn.fits\n",
    "asn_contents = Table.read(asnfile) # Gets the contents of the asn file\n",
    "asn_contents # Display these contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the association file has five rows: four exposures denoted with the `MEMTYPE` = `EXP-FP`, and a product with `MEMTYPE` = `PROD-FP`.\n",
    "\n",
    "In the cell below, we examine a bit about each of the exposures as a diagnostic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for memname, memtype in zip(asn_contents['MEMNAME'], asn_contents[\"MEMTYPE\"]): #looks through each file in asn table\n",
    "    memname = memname.lower() # get file names in lower case letters\n",
    "    if memtype == 'EXP-FP': # We only want to look at the exposure files\n",
    "        rt_a = (glob.glob(f\"**/*{memname}*rawtag_a*\", recursive=True))[0] # Get the actual filepath of the memname for rawtag_a and rawtag_b\n",
    "        rt_b = (glob.glob(f\"**/*{memname}*rawtag_b*\", recursive=True))[0]\n",
    "        # Now print all these diagnostics:\n",
    "        print(f\"Association {(fits.getheader(rt_a))['ASN_ID']} has {memtype} exposure {memname.upper()} with \\\n",
    "exptime {(fits.getheader(rt_a, ext = 1))['EXPTIME']} seconds at cenwave {(fits.getheader(rt_a, ext = 0))['CENWAVE']} Ã… and FP-POS {(fits.getheader(rt_a, ext = 0))['FPPOS']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Something seems amiss with exposure LDIF01TYQ!\n",
    "This file has an exposure time of 0.0 seconds - something has gone wrong. In this case, there was a guide star acquisition failure as described on the [data preview page](http://archive.stsci.edu/cgi-bin/mastpreview?mission=hst&dataid=LDIF01010).\n",
    "\n",
    "In the next section, we will work to correct this lack of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = editAF></a>\n",
    "# 2. Editing an existing association file\n",
    "\n",
    "<a id = subAF></a>\n",
    "## 2.1. Removing an exposure\n",
    "\n",
    "We know that at least one of our exposures - `ldif01tyq` - is not suited for combination into the final product. It has an exposure time of 0.0 seconds, in this case from a guide star acquisition failure. This is a generalizable issue, as you may often know an exposure is \"*bad*\" for many reasons: perhaps they were taken with the shutter closed, or with anomolously high background noise, or any number of reasons we may wish to exclude them from our data. To do this, we will need to alter our existing association file before we re-run `calcos`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again see the contents of our main association file below. Note that `True/False` and `1/0` are essentially interchangable in the `MEMPRSNT` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(asnfiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the `MEMPRSNT` value to `False` or `0` for our bad exposure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(asnfile, mode = 'update') as hdulist: # We need to change things with the asnfile opened and in 'update' mode\n",
    "    tbdata = hdulist[1].data # This is where the table data is\n",
    "    for expfile in tbdata: # Check if each file is one of the bad ones\n",
    "        if expfile['MEMNAME'] in ['LDIF01TYQ']:\n",
    "            expfile['MEMPRSNT'] = False # If so, set MEMPRSNT to False AKA 0\n",
    "Table.read(asnfile) # Re-read the table to see the change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = addAF></a>\n",
    "## 2.2. Adding an exposure\n",
    "We removed the failed exposure taken with `FP-POS = 1`. Usually we want to combine one of each of the four [*fixed-pattern noise positions* (`FP-POS`)](https://hst-docs.stsci.edu/cosdhb/chapter-1-cos-overview/1-1-instrument-capabilities-and-design), so lets add the `FP-POS = 1` exposure from the other association group.\n",
    "\n",
    "In the cell below, we determine which exposure this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_contents_2 = Table.read(asnfiles[1]) # Gets the contents of the SECOND asn file\n",
    "\n",
    "for memname, memtype in zip(asn_contents_2['MEMNAME'], asn_contents_2[\"MEMTYPE\"]): #looks through each file in asn table\n",
    "    memname = memname.lower() # get file names in lower case letters\n",
    "    if memtype == 'EXP-FP': # We only want to look at the exposure files\n",
    "        rt_a = (glob.glob(f\"**/*{memname}*rawtag_a*\", recursive=True))[0] # Get the actual filepath of the memname for rawtag_a and rawtag_b\n",
    "        rt_b = (glob.glob(f\"**/*{memname}*rawtag_b*\", recursive=True))[0]\n",
    "        # Now print all these diagnostics:\n",
    "        print(f\"Association {(fits.getheader(rt_a))['ASN_ID']} has {memtype} exposure {memname.upper()} with \\\n",
    "exptime {(fits.getheader(rt_a, ext = 1))['EXPTIME']} seconds at cenwave {(fits.getheader(rt_a, ext = 0))['CENWAVE']} Ã… and FP-POS {(fits.getheader(rt_a, ext = 0))['FPPOS']}.\")\n",
    "\n",
    "        if (fits.getheader(rt_a, ext = 0))['FPPOS'] == 1:\n",
    "            print(f\"^^^ The one above this has the right FP-POS! ({memname.upper()})^^^\")\n",
    "            asn2_fppos1_name = memname.upper() # save the right file basename to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a slightly different procedure to add a new exposure to the list rather than remove one. \n",
    "\n",
    "Here we want to read the table in the fits association file into an `astropy` Table. We can then add a row into the right spot, filling it with the new file's `MEMNAME`, `MEMTYPE`, and `MEMPRSNT`. Finally, we have to save this table into the existing fits association file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_orig_table = Table.read(asnfile) # Read in original data from the file\n",
    "asn_orig_table.insert_row(len(asn_orig_table)- 1 , [asn2_fppos1_name,'EXP-FP',1]) # add a row with the right name after all the original EXP-FP's\n",
    "new_table = fits.BinTableHDU(asn_orig_table)\n",
    "\n",
    "with fits.open(asnfile, mode = 'update') as hdulist: # We need to change things with the asnfile opened and in 'update' mode\n",
    "    hdulist[1].data = new_table.data  # Change the orig file's data to the new table data we made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see there is a new row with our exposure from the other `asn` file group: `LDIF02NWQ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(asnfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excellent! In the next section we will create a new association file from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = newAF></a>\n",
    "# 3. Creating an entirely new association file\n",
    "\n",
    "For the sake of demonstration, we will generate a new association file with four exposure members: even-numbered `FP-POS` (2,4) from the first original association (`LDIF01010`), and odd-numbered `FP-POS` (1,3) from from the second original association (`LDIF02010`).\n",
    "\n",
    "From section 2, we see that this corresponds to :\n",
    "\n",
    "|Name|Original asn|FP-POS|\n",
    "|----|------------|------|\n",
    "|LDIF02010|LDIF02NMQ|1|\n",
    "|LDIF01010|LDIF01U0Q|2|\n",
    "|LDIF02010|LDIF02NUQ|3|\n",
    "|LDIF01010|LDIF01U4Q|4|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = simpleAF></a>\n",
    "## 3.1. Simplest method\n",
    "Below, we manually build up an association file from the three necessary columns:\n",
    "1. `MEMNAME`\n",
    "2. `MEMTYPE`\n",
    "3. `MEMPRSNT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the exposure file details to the association table\n",
    "new_asn_memnames = ['LDIF02NMQ','LDIF01U0Q','LDIF02NUQ','LDIF01U4Q'] # MEMNAME\n",
    "types = ['EXP-FP', 'EXP-FP', 'EXP-FP', 'EXP-FP'] # MEMTYPE\n",
    "included = [True, True, True, True] # MEMPRSNT\n",
    "\n",
    "# Adding the ASN details to the end of the association table\n",
    "new_asn_memnames.append('ldifcombo'.upper()) # MEMNAME\n",
    "types.append('PROD-FP') # MEMTYPE\n",
    "included.append(True) # MEMPRSNT\n",
    "\n",
    "# Putting together the fits table\n",
    "#   40 is the number of characters allowed in this field. If your rootname is longer than 40, \n",
    "#     you will need to increase this\n",
    "c1 = fits.Column(name='MEMNAME', array=np.array(new_asn_memnames), format='40A') \n",
    "c2 = fits.Column(name='MEMTYPE', array=np.array(types), format='14A')\n",
    "c3 = fits.Column(name='MEMPRSNT', format='L', array=included)\n",
    "asn_table = fits.BinTableHDU.from_columns([c1, c2, c3])\n",
    "\n",
    "# Writing the fits table\n",
    "asn_table.writeto(output_dir + 'ldifcombo_asn.fits', overwrite = True)\n",
    "\n",
    "print('Saved: '+ 'ldifcombo_asn.fits'+ f\" in the output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the file we have created:\n",
    "##### We see that the data looks great - exactly the table we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(output_dir + 'ldifcombo_asn.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### However, the 0th and 1st fits headers no longer contain useful information about the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits.getheader(output_dir + 'ldifcombo_asn.fits', ext = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits.getheader(output_dir + 'ldifcombo_asn.fits', ext = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = metaAF></a>\n",
    "## 3.2. With fits header metadata\n",
    "\n",
    "#### We can instead build up a new file with our old file's fits header, and alter it to reflect our changes.\n",
    "We first build a new association file, a Frankenstein-esque combination of our original file's headers and our new table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(asnfile, mode = 'readonly') as hdulist: # Open up the old asn file\n",
    "    hdulist.info() # Shows the first hdu is empty except for the header we want\n",
    "    hdu0 = hdulist[0] # We want to directly copy over the old 0th hdu\n",
    "    d0 = hdulist[0].data # Unsure why this access would be necessary but seems to allow the readout \n",
    "    h1 = hdulist[1].header # just copy over the old header from 1st hdu\n",
    "    \n",
    "hdu1 = fits.BinTableHDU.from_columns([c1, c2, c3], header = h1) # Put together new 1st hdu from old header and new data\n",
    "\n",
    "new_HDUlist = fits.HDUList([hdu0,hdu1]) # New HDUList from old HDU 0 and new combined HDU 1\n",
    "new_HDUlist.writeto(output_dir + 'ldifcombo_2_asn.fits', overwrite = True) # Write this out to a new file\n",
    "new_asnfile = output_dir + 'ldifcombo_2_asn.fits' # Path to this new file\n",
    "print('\\nSaved: '+ 'ldifcombo_2_asn.fits'+ f\"in the output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we edit the relevant values in our fits headers that are different from the original.\n",
    "*Note: It is possible that a generic fits file may have different values you may wish to change. It is highly recommended to examine your fits headers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.date.today() # Grab today's date\n",
    "# Below, make a dict of what header values we want to change, corresponding to [new value , extension the value lives in, 2nd extension if applies]\n",
    "keys_to_change = {'DATE':[f'{date.year}-{date.month}-{date.day}',0], 'FILENAME':['ldifcombo_2_asn.fits',0],\n",
    "                      'ROOTNAME':['ldifcombo_2',0,1], 'ASN_ID':['ldifcombo_2',0], 'ASN_TAB':['ldifcombo_2_asn.fits',0], 'ASN_PROD':['False',0],\n",
    "                     'EXTVER':[2,1], 'EXPNAME':['ldifcombo_2',1]}\n",
    "# Actually change the values below (verbosely):\n",
    "for keyval in keys_to_change.items():\n",
    "    print(f\"Editing {keyval[0]} in Extension {keyval[1][1]}\")\n",
    "    fits.setval(filename=new_asnfile,keyword= keyval[0],value= keyval[1][0], ext = keyval[1][1])\n",
    "    # Below is necessary as some keys are repeated in both headers ('ROOTNAME')\n",
    "    if len(keyval[1])>2:\n",
    "        print(f\"Editing {keyval[0]} in Extension {keyval[1][2]}\")\n",
    "        fits.setval(filename=new_asnfile,keyword= keyval[0],value= keyval[1][0], ext = keyval[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And there you have it - the new association file is all set and ready to be used in the `calcos` pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You finished this notebook!\n",
    "### There are more COS data walkthrough notebooks on different topics. You can find them [here](https://github.com/spacetelescope/COS-Notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## About this Notebook\n",
    "**Author:** Nat Kerman: <nkerman@stsci.edu>\n",
    "\n",
    "**Contributors:** Elaine Mae Frazer\n",
    "\n",
    "**Updated On:** 2020-11-16\n",
    "\n",
    "> *This tutorial was generated to be in compliance with the [STScI style guides](https://github.com/spacetelescope/style-guides) and would like to cite the [Jupyter guide](https://github.com/spacetelescope/style-guides/blob/master/templates/example_notebook.ipynb) in particular.*\n",
    "\n",
    "## Citations\n",
    "\n",
    "If you use `astropy`, `matplotlib`, `astroquery`, or `numpy` for published research, please cite the\n",
    "authors. Follow these links for more information about citations:\n",
    "\n",
    "* [Citing `astropy`/`numpy`/`matplotlib`](https://www.scipy.org/citing.html)\n",
    "* [Citing `astroquery`](https://astroquery.readthedocs.io/en/latest/)\n",
    "\n",
    "---\n",
    "\n",
    "[Top of Page](#topAF)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> \n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
